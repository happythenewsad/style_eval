{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(42192, 1645)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "LABEL_COL = -1 \n",
    "\n",
    "with open('raw_all_plays_1645.pickle', 'rb') as f:\n",
    "    raw = pickle.load(f)\n",
    "\n",
    "with open('X_all_plays_1645.pickle', 'rb') as f:\n",
    "    X = pickle.load(f)\n",
    "X.shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10548.0\n",
      "total rows: 42192 train val split delim: 8438 val test split delim: 9493\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1.5, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "import numpy as np\n",
    "np.set_printoptions(suppress=True)\n",
    "\n",
    "import time\n",
    "import math\n",
    "###############\n",
    "# training ####\n",
    "###############\n",
    "\n",
    "n = len(X)*.25\n",
    "print(n)\n",
    "train_val_split_delim = math.floor(n * .8)\n",
    "val_test_split_delim = math.floor(n * .9)\n",
    "end_delim = int(n)\n",
    "\n",
    "\n",
    "print(\"total rows: {} train val split delim: {} val test split delim: {}\".format(len(X), train_val_split_delim, val_test_split_delim))\n",
    "# numpy note: addressing [:delim] is exclusive of delim, but\n",
    "#                         [delim:] is inclusive of delim\n",
    "\n",
    "\n",
    "X_train = X[0:train_val_split_delim]\n",
    "Y_train = raw[0:train_val_split_delim,LABEL_COL]\n",
    "\n",
    "X_val = X[train_val_split_delim:val_test_split_delim]\n",
    "Y_val = raw[train_val_split_delim:val_test_split_delim,LABEL_COL]\n",
    "\n",
    "X_test = X[val_test_split_delim:end_delim]\n",
    "Y_test = raw[val_test_split_delim:end_delim,LABEL_COL]\n",
    "\n",
    "if len(X_test) != len(Y_test):\n",
    "    raise ValueError(\"mismatched arrays! {} {}\".format(len(preds), len(y_true)))\n",
    "\n",
    "    \n",
    "# Utility function to report best scores (from sklearn example documentation)\n",
    "def report(results, n_top=3):\n",
    "    for i in range(1, n_top + 1):\n",
    "        candidates = np.flatnonzero(results['rank_test_score'] == i)\n",
    "        for candidate in candidates:\n",
    "            print(\"Model with rank: {0}\".format(i))\n",
    "            print(\"Mean validation score: {0:.3f} (std: {1:.3f})\".format(\n",
    "                  results['mean_test_score'][candidate],\n",
    "                  results['std_test_score'][candidate]))\n",
    "            print(\"Parameters: {0}\".format(results['params'][candidate]))\n",
    "            print(\"\")\n",
    "            \n",
    "            \n",
    "param_grid = {\"alpha\": [.25,.5,1,1.5,2]}#\n",
    "mdl = MultinomialNB(alpha=1.5)\n",
    "mdl.fit(X_train, Y_train)\n",
    "# grid_search = GridSearchCV(mdl, param_grid=param_grid)\n",
    "# grid_search.fit(X_train, Y_train)\n",
    "# report(grid_search.cv_results_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def persist_features(X, name='X_all_plays.pickle'):\n",
    "#     with open(name, 'wb') as f:\n",
    "#         pickle.dump(X, f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "#persist_features(mdl, 'mdl_all_plays_pos_7_gram.pickle')\n",
    "# persist_features(X, 'X_all_plays_pos_gram.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Accuracy: 0.6085308056872037\n",
      "Predictions evaluated: 1055\n",
      "test Accuracy: 0.6009478672985782\n"
     ]
    }
   ],
   "source": [
    "# evaluation ###\n",
    "################\n",
    "#second class is original\n",
    "\n",
    "\n",
    "#y_true = raw[train_val_split_delim:val_test_split_delim,LABEL_COL]\n",
    "val_preds = mdl.predict(X_val)\n",
    "\n",
    "#y_true = [int(raw[int(idx)][-1]) for idx in val_indices]\n",
    "\n",
    "# brier = brier_score_loss(y_true, preds[:,0]) \n",
    "# print(\"brier score:\", brier)\n",
    "\n",
    "#currently slightly worse than randomly permuting preds\n",
    "\n",
    "def accuracy(preds, y_true):\n",
    "    if len(preds) != len(y_true):\n",
    "        raise ValueError(\"mismatched arrays! {} {}\".format(len(preds), len(y_true)))\n",
    "    num_correct = 0\n",
    "    for (x,y) in zip(preds, y_true):\n",
    "        if x == y:\n",
    "            num_correct = num_correct + 1\n",
    "    return num_correct/len(preds)\n",
    "\n",
    "print(\"val Accuracy: {}\".format(accuracy(val_preds,Y_val)))\n",
    "print(\"Predictions evaluated: {}\".format(len(val_preds)))\n",
    "\n",
    "test_preds = mdl.predict(X_test)\n",
    "print(\"test Accuracy: {}\".format(accuracy(test_preds,Y_test)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'preds' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-7782233fd202>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mitertools\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mconf_mat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfusion_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;31m# fig, ax = plt.subplots(figsize=(10,10))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# # sns.heatmap(conf_mat, annot=True, fmt='d',\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'preds' is not defined"
     ]
    }
   ],
   "source": [
    "# error analysis ##\n",
    "##################\n",
    "\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "\n",
    "conf_mat = confusion_matrix(Y_val, preds)\n",
    "# fig, ax = plt.subplots(figsize=(10,10))\n",
    "# # sns.heatmap(conf_mat, annot=True, fmt='d',\n",
    "# #             xticklabels=category_id_df.Product.values, yticklabels=category_id_df.Product.values)\n",
    "# plt.ylabel('Actual')\n",
    "# plt.xlabel('Predicted')\n",
    "\n",
    "# NOTE: this function taken from: \n",
    "# http://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html#sphx-glr-auto-examples-model-selection-plot-confusion-matrix-py\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f'\n",
    "    thresh = 500\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "\n",
    "\n",
    "plt.figure()\n",
    "print(conf_mat)\n",
    "plot_confusion_matrix(conf_mat, classes=mdl.classes_,\n",
    "                      title=\"Confusion matrix\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_len = len(preds)\n",
    "# accuracy = sum(preds)/float(test_len)\n",
    "# print(\"accuracy: {}\".format(accuracy))\n",
    "\n",
    "#othello accuracy: 0.7428571428571429. but maybe not permuted\n",
    "#all: accuracy: 0.328237942884228\n",
    "\n",
    "#pos, no bleu, 2 plays\n",
    "# Mult NB: .519"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# doc = nlp(u\"a sleep that ends all the heartache\")\n",
    "# # span = doc[doc[4].left_edge.i : doc[4].right_edge.i+1]\n",
    "# # span.merge()\n",
    "# for token in doc:\n",
    "#     print(token.text, token.pos_, token.dep_, token.head.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#raw[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "int(2.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
