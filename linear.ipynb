{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unidecode\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "import random as rd\n",
    "\n",
    "play_file_infixes = ['othello', 'antony-and-cleopatra', 'asyoulikeit', \n",
    "                     'errors', 'hamlet', 'henryv', 'juliuscaesar', 'lear', 'macbeth', \n",
    "                     'merchant', 'msnd', 'muchado', 'richardiii', 'romeojuliet', \n",
    "                     'shrew', 'tempest', 'twelfthnight']\n",
    "play_file_infixes = ['othello']\n",
    "\n",
    "agg_original_tuples = []\n",
    "agg_modern_tuples = []\n",
    "path = \"data/shakespeare/data/align/plays/merged/\"\n",
    "\n",
    "for infix in play_file_infixes:\n",
    "    modern_tuples = unidecode.unidecode(open(path + infix + \"_modern.snt.aligned\").read()).split(\"\\n\")\n",
    "    original_tuples = unidecode.unidecode(open(path + infix + \"_original.snt.aligned\").read()).split(\"\\n\")\n",
    "    agg_original_tuples.extend(original_tuples)\n",
    "    agg_modern_tuples.extend(modern_tuples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What's the matter, lieutenant?\n",
      "What's the matter, lieutenant?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"{}\\n{}\\n\".format(agg_original_tuples[0], agg_modern_tuples[0]))\n",
    "#print(\"{}\\n{}\\n\".format(agg_original_tuples[5000], agg_modern_tuples[5000]))\n",
    "# print(\"{}\\n{}\\n\".format(agg_original_tuples[9000], agg_modern_tuples[9000]))\n",
    "# print(\"{}\\n{}\\n\".format(agg_original_tuples[18000], agg_modern_tuples[18000]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3710, 1)\n",
      "(3710, 2)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(3710, 3)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# in-class = original = '1'\n",
    "# raw -> (raw text, label)\n",
    "import numpy as np\n",
    "\n",
    "style1_np = np.array(agg_original_tuples)\n",
    "style1_labels = np.array(['1' for x in range(len(agg_original_tuples))])\n",
    "X = np.vstack((style1_np, style1_labels))\n",
    "\n",
    "style2_np = np.array(agg_modern_tuples)\n",
    "style2_labels = np.array(['0' for x in range(len(agg_modern_tuples))])\n",
    "X2 = np.vstack((style2_np, style2_labels))\n",
    "\n",
    "X = np.hstack((X,X2))\n",
    "raw = np.transpose(X)\n",
    "\n",
    "idx_col = [str(x) for x in range(len(raw))]\n",
    "idx_col = np.array(idx_col).reshape(-1,1)\n",
    "print(idx_col.shape)\n",
    "print(raw.shape)\n",
    "raw = np.hstack((idx_col, raw))\n",
    "raw.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['2000', \"CASSIO  She's awake, sir.\", '0'], dtype='<U497')"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw[2000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What's the matter, lieutenant? gold: 1 {'sent_len': 30, 'NOUN_count': 3, 'ADV_count': 0, 'VERB_count': 1, 'ADJ_count': 0, 'adv_verb_ratio': 0, 'adj_noun_ratio': 0, 'DET_count': 1, 'PUNCT_count': 2}\n",
      "\n",
      "Therefore, good Emilia, Give me my nightly wearing, and adieu. gold: 1 {'sent_len': 62, 'NOUN_count': 1, 'ADV_count': 2, 'VERB_count': 2, 'ADJ_count': 2, 'adv_verb_ratio': 1.0, 'adj_noun_ratio': 0.5, 'PUNCT_count': 4, 'PROPN_count': 1, 'PRON_count': 1, 'CCONJ_count': 1}\n",
      "\n",
      "list index out of range\n",
      "offending message [], len: 0, row idx: 1854\n",
      "CASSIO  She's awake, sir. gold: 0 {'sent_len': 25, 'NOUN_count': 1, 'ADV_count': 0, 'VERB_count': 1, 'ADJ_count': 1, 'adv_verb_ratio': 0, 'adj_noun_ratio': 1.0, 'PROPN_count': 1, 'SPACE_count': 1, 'PRON_count': 1, 'PUNCT_count': 2}\n",
      "\n",
      "Who's shouting? gold: 0 {'sent_len': 15, 'NOUN_count': 1, 'ADV_count': 0, 'VERB_count': 2, 'ADJ_count': 0, 'adv_verb_ratio': 0, 'adj_noun_ratio': 0, 'PUNCT_count': 1}\n",
      "\n",
      "list index out of range\n",
      "offending message [], len: 0, row idx: 3709\n",
      "<class 'scipy.sparse.csr.csr_matrix'>\n",
      "(3710, 18)\n",
      "(3710, 1)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction import DictVectorizer\n",
    "import spacy\n",
    "import scipy.sparse\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "\n",
    "vectorizer = DictVectorizer()\n",
    "features = []\n",
    "\n",
    "# feature engineering\n",
    "    # construct index column\n",
    "idx_col = np.array([raw[:,0]])\n",
    "idx_col = idx_col.astype('f', copy=True)\n",
    "\n",
    "#bleu: 0-1, 1 is perfect score\n",
    "\n",
    "bleu = sentence_bleu([sen2],sen1)\n",
    "\n",
    "raw_text_col = 1\n",
    "LABEL_COL = -1\n",
    "\n",
    "for row_idx,_ in enumerate(raw):\n",
    "    feature_dict = {}\n",
    "    pos_dict = {'NOUN_count': 0, 'ADV_count': 0, 'VERB_count': 0, \n",
    "                'ADJ_count': 0, 'adv_verb_ratio': 0, 'adj_noun_ratio': 0, 'bleu': 0}\n",
    "    txt = str(raw[row_idx][raw_text_col])\n",
    "    \n",
    "    feature_dict['sent_len'] = len(txt)\n",
    "    try:\n",
    "        doc = nlp(txt)\n",
    "    except IndexError as e:\n",
    "        print(e)\n",
    "        print(\"offending message [{}], len: {}, row idx: {}\".format(txt, len(txt), row_idx))\n",
    "\n",
    "    for token in doc:\n",
    "        pos = token.pos_\n",
    "        pos_dict[pos+'_count'] = pos_dict.get(pos+'_count', 0) + 1\n",
    "\n",
    "    if pos_dict['NOUN_count'] == 0 or pos_dict['ADJ_count'] == 0:\n",
    "        pos_dict['adj_noun_ratio'] = 0\n",
    "    else:\n",
    "        pos_dict['adj_noun_ratio'] = pos_dict['NOUN_count'] / pos_dict['ADJ_count'] \n",
    "\n",
    "    if pos_dict['VERB_count'] == 0 or pos_dict['ADV_count'] == 0:\n",
    "        pos_dict['adv_verb_ratio'] = 0\n",
    "    else:\n",
    "        pos_dict['adv_verb_ratio'] = pos_dict['VERB_count'] / pos_dict['ADV_count'] \n",
    "        \n",
    "    feature_dict.update(pos_dict)\n",
    "    \n",
    "    if row_idx % 1000 == 0:\n",
    "        print(\"{} gold: {} {}\\n\".format(txt, raw[row_idx][LABEL_COL], feature_dict) )\n",
    "        \n",
    "    features.append(feature_dict)\n",
    "\n",
    "labels_col = np.array([raw[:,LABEL_COL].astype(float)])\n",
    "\n",
    "X = vectorizer.fit_transform(features)\n",
    "print(type(X))\n",
    "print(X.shape)\n",
    "print(idx_col.T.shape)\n",
    "X = scipy.sparse.hstack(( idx_col.T, X, labels_col.T ))\n",
    "X = X.todense()\n",
    "# X = np.transpose(X)\n",
    "# X = X.astype('f', copy=True)\n",
    "# print(X[0:5])\n",
    "# print(X[3700:3705])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'It were an honest action to say So to the Moor.'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw[1853][raw_text_col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[ 1.,  0.,  1.,  0.,  0.,  2.,  0.,  3.,  0.,  0.,  1.,  0.,  1.,\n",
       "          0.,  2.,  0.,  0.,  0., 42.,  1.]])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[1:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total rows: 3710 train val split delim: 2968 val test split delim: 3339\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "import time\n",
    "import math\n",
    "###############\n",
    "# training ####\n",
    "###############\n",
    "# test breakout: 3500, 210\n",
    "train_val_split_delim = math.floor(len(X) * .8)\n",
    "val_test_split_delim = math.floor(len(X) * .9)\n",
    "print(\"total rows: {} train val split delim: {} val test split delim: {}\".format(len(X), train_val_split_delim, val_test_split_delim))\n",
    "# numpy note: addressing [:delim] is exclusive of delim, but\n",
    "#                         [delim:] is inclusive of delim\n",
    "last_feat_col = X.shape[1] -2\n",
    "X = np.random.permutation(X)\n",
    "\n",
    "X_train = X[0:train_val_split_delim,1:last_feat_col ]\n",
    "Y_train = X[0:train_val_split_delim,LABEL_COL]\n",
    "\n",
    "X_val = X[train_val_split_delim:val_test_split_delim,1:last_feat_col ]\n",
    "Y_val = X[train_val_split_delim:val_test_split_delim,LABEL_COL]\n",
    "\n",
    "X_test = X[val_test_split_delim:,1:last_feat_col ]\n",
    "Y_test = X[val_test_split_delim:,LABEL_COL]\n",
    "\n",
    "\n",
    "#mdl = LogisticRegression(random_state=123)\n",
    "mdl = MultinomialNB()\n",
    "mdl.fit(X_train, Y_train)\n",
    "\n",
    "np.set_printoptions(suppress=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 1.])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mdl.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "brier score: 0.2776796938888296\n"
     ]
    }
   ],
   "source": [
    "# evaluation ###\n",
    "################\n",
    "#second class is original\n",
    "\n",
    "from sklearn.metrics import brier_score_loss\n",
    "\n",
    "val_indices = X[train_val_split_delim:val_test_split_delim,0]\n",
    "\n",
    "# def brier_score(raw, preds, val_indices, loss='SSE'):\n",
    "#     if len(preds) != len(val_indices):\n",
    "#         raise ArgumentError(\"weighted_eval: list length mismatch!\")\n",
    "        \n",
    "#     results = []\n",
    "    \n",
    "#     for (idx, pred) in zip(val_indices, preds):\n",
    "#         gold_label = int(raw[int(idx)][-1])\n",
    "        \n",
    "#         #pred_label = int(pred)\n",
    "#         #print(\"int'd idx: {} gold label: {} pred label: {}\".format(int(idx), gold_label, pred_label))\n",
    "\n",
    "#         predicted_class = 0 if (pred[0] > pred[1]) else 1\n",
    "        \n",
    "#         if gold_label == pred_label:\n",
    "#               results.append( (1 - pred[predicted_class])**2 )\n",
    "#         else:\n",
    "#               results.append( (pred[predicted_class])**2 )\n",
    "#         print(\"{} | gold: {} predicted: {}\".format(raw[idx][1], gold_label, pred ))\n",
    "#     return sum(results)/len(preds)\n",
    "    \n",
    "#binary evaluation\n",
    "# preds = mdl.predict(X_test)\n",
    "# \n",
    "# test_indices = X[val_test_split_delim:,0]\n",
    "# if len(preds) != len(val_indices):\n",
    "#     raise ValueError(\"unmatched arrays\")\n",
    "    \n",
    "# results = []\n",
    "# for (idx, pred) in zip(val_indices,preds):\n",
    "#     gold_label = int(raw[int(idx)][-1])\n",
    "#     pred_label = int(pred)\n",
    "#     #print(\"int'd idx: {} gold label: {} pred label: {}\".format(int(idx), gold_label, pred_label))\n",
    "    \n",
    "#     if gold_label == pred_label:\n",
    "#           results.append(1)\n",
    "#     else:\n",
    "#         results.append(0)\n",
    "# results[:5]\n",
    "\n",
    "preds = mdl.predict_proba(X_test)\n",
    "\n",
    "y_true = [int(raw[int(idx)][-1]) for idx in val_indices]\n",
    "\n",
    "brier = brier_score_loss(y_true, preds[:,0]) \n",
    "print(\"brier score:\", brier)\n",
    "\n",
    "#currently slightly worse than randomly permuting preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_len = len(preds)\n",
    "# accuracy = sum(preds)/float(test_len)\n",
    "# print(\"accuracy: {}\".format(accuracy))\n",
    "\n",
    "#othello accuracy: 0.7428571428571429. but maybe not permuted\n",
    "#all: accuracy: 0.328237942884228\n",
    "\n",
    "#pos, no bleu, 2 plays\n",
    "# Mult NB: .519"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
