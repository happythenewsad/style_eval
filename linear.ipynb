{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unidecode\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "import random as rd\n",
    "\n",
    "play_file_infixes = ['othello', 'antony-and-cleopatra', 'asyoulikeit', \n",
    "                     'errors', 'hamlet', 'henryv', 'juliuscaesar', 'lear', 'macbeth', \n",
    "                     'merchant', 'msnd', 'muchado', 'richardiii', 'romeojuliet', \n",
    "                     'shrew', 'tempest', 'twelfthnight']\n",
    "play_file_infixes = ['othello', 'richardiii']\n",
    "\n",
    "agg_original_tuples = []\n",
    "agg_modern_tuples = []\n",
    "path = \"data/shakespeare/data/align/plays/merged/\"\n",
    "\n",
    "for infix in play_file_infixes:\n",
    "    modern_tuples = unidecode.unidecode(open(path + infix + \"_modern.snt.aligned\").read()).split(\"\\n\")\n",
    "    original_tuples = unidecode.unidecode(open(path + infix + \"_original.snt.aligned\").read()).split(\"\\n\")\n",
    "    agg_original_tuples.extend(original_tuples)\n",
    "    agg_modern_tuples.extend(modern_tuples)\n",
    "# aligned_othello_style1 = \"data/shakespeare/data/align/plays/merged/othello_modern.snt.aligned\"\n",
    "# aligned_othello_style2 = \"data/shakespeare/data/align/plays/merged/othello_original.snt.aligned\"\n",
    "\n",
    "# aligned_ant_cleo_style1 = \"data/shakespeare/data/align/plays/merged/antony-and-cleopatra_modern.snt.aligned\"\n",
    "# aligned_ant_cleo_style2 = \"data/shakespeare/data/align/plays/merged/antony-and-cleopatra_original.snt.aligned\"\n",
    "\n",
    "# aligned_asyoulikeit_style1 = \"data/shakespeare/data/align/plays/merged/asyoulikeit_modern.snt.aligned\"\n",
    "# aligned_asyoulikeit_style2 = \"data/shakespeare/data/align/plays/merged/asyoulikeit_original.snt.aligned\"\n",
    "\n",
    "# aligned_errors_style1 = \"data/shakespeare/data/align/plays/merged/errors_modern.snt.aligned\"\n",
    "# aligned_errors_style2 = \"data/shakespeare/data/align/plays/merged/errors_original.snt.aligned\"\n",
    "\n",
    "# aligned_hamlet_style1 = \"data/shakespeare/data/align/plays/merged/hamlet_modern.snt.aligned\"\n",
    "# aligned_hamlet_style2 = \"data/shakespeare/data/align/plays/merged/hamlet_original.snt.aligned\"\n",
    "\n",
    "# aligned_henryv_style1 = \"data/shakespeare/data/align/plays/merged/henryv_modern.snt.aligned\"\n",
    "# aligned_henryv_style2 = \"data/shakespeare/data/align/plays/merged/henryv_original.snt.aligned\"\n",
    "\n",
    "# aligned_juliuscaesar_style1 = \"data/shakespeare/data/align/plays/merged/juliuscaesar_modern.snt.aligned\"\n",
    "# aligned_juliuscaesar_style2 = \"data/shakespeare/data/align/plays/merged/juliuscaesar_original.snt.aligned\"\n",
    "\n",
    "# style1_tuples = unidecode.unidecode(open(aligned_style1).read())\n",
    "# style2_tuples = unidecode.unidecode(open(aligned_style2).read())\n",
    "\n",
    "# style1_tuples = style1_tuples.split(\"\\n\")\n",
    "# style2_tuples = style2_tuples.split(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"{}\\n{}\\n\".format(agg_original_tuples[0], agg_modern_tuples[0]))\n",
    "# print(\"{}\\n{}\\n\".format(agg_original_tuples[5000], agg_modern_tuples[5000]))\n",
    "# print(\"{}\\n{}\\n\".format(agg_original_tuples[9000], agg_modern_tuples[9000]))\n",
    "# print(\"{}\\n{}\\n\".format(agg_original_tuples[18000], agg_modern_tuples[18000]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6566, 1)\n",
      "(6566, 2)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(6566, 3)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# in-class = original = '1'\n",
    "# raw -> (raw text, label)\n",
    "import numpy as np\n",
    "\n",
    "style1_np = np.array(agg_original_tuples)\n",
    "style1_labels = np.array(['1' for x in range(len(agg_original_tuples))])\n",
    "X = np.vstack((style1_np, style1_labels))\n",
    "\n",
    "style2_np = np.array(agg_modern_tuples)\n",
    "style2_labels = np.array(['0' for x in range(len(agg_modern_tuples))])\n",
    "X2 = np.vstack((style2_np, style2_labels))\n",
    "\n",
    "X = np.hstack((X,X2))\n",
    "raw = np.transpose(X)\n",
    "\n",
    "idx_col = [str(x) for x in range(len(raw))]\n",
    "idx_col = np.array(idx_col).reshape(-1,1)\n",
    "print(idx_col.shape)\n",
    "print(raw.shape)\n",
    "raw = np.hstack((idx_col, raw))\n",
    "raw.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['2000', 'I long with all my heart to see the prince.', '1'],\n",
       "      dtype='<U497')"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw[2000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What's the matter, lieutenant? gold: 1 {'sent_len': 30, 'NOUN_count': 3, 'ADV_count': 0, 'VERB_count': 1, 'ADJ_count': 0, 'adv_verb_ratio': 0, 'adj_noun_ratio': 0, 'DET_count': 1, 'PUNCT_count': 2}\n",
      "\n",
      "Therefore, good Emilia, Give me my nightly wearing, and adieu. gold: 1 {'sent_len': 62, 'NOUN_count': 1, 'ADV_count': 2, 'VERB_count': 2, 'ADJ_count': 2, 'adv_verb_ratio': 1.0, 'adj_noun_ratio': 0.5, 'PUNCT_count': 4, 'PROPN_count': 1, 'PRON_count': 1, 'CCONJ_count': 1}\n",
      "\n",
      "I long with all my heart to see the prince. gold: 1 {'sent_len': 43, 'NOUN_count': 2, 'ADV_count': 1, 'VERB_count': 1, 'ADJ_count': 2, 'adv_verb_ratio': 1.0, 'adj_noun_ratio': 1.0, 'PRON_count': 1, 'ADP_count': 1, 'PART_count': 1, 'DET_count': 1, 'PUNCT_count': 1}\n",
      "\n",
      "On me, that halts and am misshapen thus? gold: 1 {'sent_len': 40, 'NOUN_count': 1, 'ADV_count': 1, 'VERB_count': 2, 'ADJ_count': 0, 'adv_verb_ratio': 2.0, 'adj_noun_ratio': 0, 'ADP_count': 2, 'PRON_count': 1, 'PUNCT_count': 2, 'CCONJ_count': 1}\n",
      "\n",
      "Yes, that's for sure. gold: 0 {'sent_len': 21, 'NOUN_count': 0, 'ADV_count': 0, 'VERB_count': 1, 'ADJ_count': 1, 'adv_verb_ratio': 0, 'adj_noun_ratio': 0, 'INTJ_count': 1, 'PUNCT_count': 2, 'DET_count': 1, 'ADP_count': 1}\n",
      "\n",
      "My darling Othello! gold: 0 {'sent_len': 19, 'NOUN_count': 2, 'ADV_count': 0, 'VERB_count': 0, 'ADJ_count': 1, 'adv_verb_ratio': 0, 'adj_noun_ratio': 2.0, 'PUNCT_count': 1}\n",
      "\n",
      "Well, what do you guess? gold: 0 {'sent_len': 24, 'NOUN_count': 1, 'ADV_count': 0, 'VERB_count': 2, 'ADJ_count': 0, 'adv_verb_ratio': 0, 'adj_noun_ratio': 0, 'INTJ_count': 1, 'PUNCT_count': 2, 'PRON_count': 1}\n",
      "\n",
      "<class 'scipy.sparse.csr.csr_matrix'>\n",
      "(6566, 18)\n",
      "(6566, 1)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction import DictVectorizer\n",
    "import spacy\n",
    "import scipy.sparse\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "print\n",
    "\n",
    "vectorizer = DictVectorizer()\n",
    "features = []\n",
    "\n",
    "# feature engineering\n",
    "    # construct index column\n",
    "idx_col = np.array([raw[:,0]])\n",
    "idx_col = idx_col.astype('f', copy=True)\n",
    "\n",
    "#sent_len_feature = np.array([-1 for x in range(len(raw))])\n",
    "raw_text_col = 1\n",
    "LABEL_COL = -1\n",
    "\n",
    "for row_idx,_ in enumerate(raw):\n",
    "    feature_dict = {}\n",
    "    pos_dict = {'NOUN_count': 0, 'ADV_count': 0, 'VERB_count': 0, \n",
    "                'ADJ_count': 0, 'adv_verb_ratio': 0, 'adj_noun_ratio': 0}\n",
    "    txt = str(raw[row_idx][raw_text_col])\n",
    "    \n",
    "    feature_dict['sent_len'] = len(txt)\n",
    "    doc = nlp(txt)\n",
    "    for token in doc:\n",
    "        pos = token.pos_\n",
    "        pos_dict[pos+'_count'] = pos_dict.get(pos+'_count', 0) + 1\n",
    "\n",
    "    if pos_dict['NOUN_count'] == 0 or pos_dict['ADJ_count'] == 0:\n",
    "        pos_dict['adj_noun_ratio'] = 0\n",
    "    else:\n",
    "        pos_dict['adj_noun_ratio'] = pos_dict['NOUN_count'] / pos_dict['ADJ_count'] \n",
    "\n",
    "    if pos_dict['VERB_count'] == 0 or pos_dict['ADV_count'] == 0:\n",
    "        pos_dict['adv_verb_ratio'] = 0\n",
    "    else:\n",
    "        pos_dict['adv_verb_ratio'] = pos_dict['VERB_count'] / pos_dict['ADV_count'] \n",
    "        \n",
    "    feature_dict.update(pos_dict)\n",
    "    \n",
    "    if row_idx % 1000 == 0:\n",
    "        print(\"{} gold: {} {}\\n\".format(txt, raw[row_idx][LABEL_COL], feature_dict) )\n",
    "        \n",
    "    features.append(feature_dict)\n",
    "\n",
    "labels_col = np.array([raw[:,LABEL_COL].astype(float)])\n",
    "\n",
    "X = vectorizer.fit_transform(features)\n",
    "print(type(X))\n",
    "print(X.shape)\n",
    "print(idx_col.T.shape)\n",
    "X = scipy.sparse.hstack(( idx_col.T, X, labels_col.T ))\n",
    "X = X.todense()\n",
    "# X = np.transpose(X)\n",
    "# X = X.astype('f', copy=True)\n",
    "# print(X[0:5])\n",
    "# print(X[3700:3705])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[ 1.,  0.,  1.,  0.,  0.,  2.,  0.,  3.,  0.,  0.,  1.,  0.,  1.,\n",
       "          0.,  2.,  0.,  0.,  0., 42.,  1.]])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[1:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total rows: 6566 train val split delim: 5252 val test split delim: 5909\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "import time\n",
    "import math\n",
    "###############\n",
    "# training ####\n",
    "###############\n",
    "# test breakout: 3500, 210\n",
    "train_val_split_delim = math.floor(len(X) * .8)\n",
    "val_test_split_delim = math.floor(len(X) * .9)\n",
    "print(\"total rows: {} train val split delim: {} val test split delim: {}\".format(len(X), train_val_split_delim, val_test_split_delim))\n",
    "# numpy note: addressing [:delim] is exclusive of delim, but\n",
    "#                         [delim:] is inclusive of delim\n",
    "last_feat_col = X.shape[1] -2\n",
    "X = np.random.permutation(X)\n",
    "\n",
    "X_train = X[0:train_val_split_delim,1:last_feat_col ]\n",
    "Y_train = X[0:train_val_split_delim,LABEL_COL]\n",
    "\n",
    "X_val = X[train_val_split_delim:val_test_split_delim,1:last_feat_col ]\n",
    "Y_val = X[train_val_split_delim:val_test_split_delim,LABEL_COL]\n",
    "\n",
    "X_test = X[val_test_split_delim:,1:last_feat_col ]\n",
    "Y_test = X[val_test_split_delim:,LABEL_COL]\n",
    "\n",
    "\n",
    "#mdl = LogisticRegression(random_state=123)\n",
    "mdl = MultinomialNB()\n",
    "mdl.fit(X_train, Y_train)\n",
    "\n",
    "np.set_printoptions(suppress=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 1.])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mdl.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.08875201783296359, 0.06207592467547917, 0.18068949024656478, 0.03350168830167186, 0.10201584225445073, 0.2812511258025143, 0.4053102290056527, 0.11757038636081334, 0.026746773362279472, 0.1947764703124586, 0.25583708594783405, 0.4185305139215705, 0.04694867223809446, 0.241283257880316, 0.118505685927454, 0.21359455454502094, 0.05506834808928139, 0.18571651931728084, 0.0012267252154180722, 0.40485019448912923]\n",
      "brier score: {} 0.29244853370098184\n"
     ]
    }
   ],
   "source": [
    "# evaluation ###\n",
    "################\n",
    "#second class is original\n",
    "\n",
    "val_indices = X[train_val_split_delim:val_test_split_delim,0]\n",
    "\n",
    "def weighted_eval(raw, preds, val_indices, loss='SSE'):\n",
    "    if len(preds) != len(val_indices):\n",
    "        raise ArgumentError(\"weighted_eval: list length mismatch!\")\n",
    "        \n",
    "    results = []\n",
    "    \n",
    "    for (idx, pred) in zip(val_indices, preds):\n",
    "        gold_label = int(raw[int(idx)][-1])\n",
    "        #pred_label = int(pred)\n",
    "        #print(\"int'd idx: {} gold label: {} pred label: {}\".format(int(idx), gold_label, pred_label))\n",
    "\n",
    "        predicted_class = 0 if (pred[0] > pred[1]) else 1\n",
    "        \n",
    "        if gold_label == pred_label:\n",
    "              results.append( (1 - pred[predicted_class])**2 )\n",
    "        else:\n",
    "              results.append( (pred[predicted_class])**2 )\n",
    "    print(results[:20])\n",
    "    return sum(results)/len(preds)\n",
    "    \n",
    "#binary evaluation\n",
    "# preds = mdl.predict(X_test)\n",
    "# \n",
    "# test_indices = X[val_test_split_delim:,0]\n",
    "# if len(preds) != len(val_indices):\n",
    "#     raise ValueError(\"unmatched arrays\")\n",
    "    \n",
    "# results = []\n",
    "# for (idx, pred) in zip(val_indices,preds):\n",
    "#     gold_label = int(raw[int(idx)][-1])\n",
    "#     pred_label = int(pred)\n",
    "#     #print(\"int'd idx: {} gold label: {} pred label: {}\".format(int(idx), gold_label, pred_label))\n",
    "    \n",
    "#     if gold_label == pred_label:\n",
    "#           results.append(1)\n",
    "#     else:\n",
    "#         results.append(0)\n",
    "# results[:5]\n",
    "preds = mdl.predict_proba(X_test)\n",
    "brier = weighted_eval(raw, preds, val_indices, loss='brier')\n",
    "print(\"brier score:\", brier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_len = len(preds)\n",
    "# accuracy = sum(preds)/float(test_len)\n",
    "# print(\"accuracy: {}\".format(accuracy))\n",
    "\n",
    "#othello accuracy: 0.7428571428571429. but maybe not permuted\n",
    "#all: accuracy: 0.328237942884228\n",
    "\n",
    "#pos, no bleu, 2 plays\n",
    "# Mult NB: .519"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
