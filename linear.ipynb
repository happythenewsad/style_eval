{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unidecode\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "import random as rd\n",
    "\n",
    "play_file_infixes = ['othello', 'antony-and-cleopatra', 'asyoulikeit', \n",
    "                     'errors', 'hamlet', 'henryv', 'juliuscaesar', 'lear', 'macbeth', \n",
    "                     'merchant', 'msnd', 'muchado', 'richardiii', 'romeojuliet', \n",
    "                     'shrew', 'tempest', 'twelfthnight']\n",
    "play_file_infixes = ['othello']\n",
    "\n",
    "agg_original_tuples = []\n",
    "agg_modern_tuples = []\n",
    "path = \"data/shakespeare/data/align/plays/merged/\"\n",
    "\n",
    "for infix in play_file_infixes:\n",
    "    modern_tuples = unidecode.unidecode(open(path + infix + \"_modern.snt.aligned\").read()).split(\"\\n\")\n",
    "    original_tuples = unidecode.unidecode(open(path + infix + \"_original.snt.aligned\").read()).split(\"\\n\")\n",
    "    agg_original_tuples.extend(original_tuples)\n",
    "    agg_modern_tuples.extend(modern_tuples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What's the matter, lieutenant?\n",
      "What's the matter, lieutenant?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"{}\\n{}\\n\".format(agg_original_tuples[0], agg_modern_tuples[0]))\n",
    "#print(\"{}\\n{}\\n\".format(agg_original_tuples[5000], agg_modern_tuples[5000]))\n",
    "# print(\"{}\\n{}\\n\".format(agg_original_tuples[9000], agg_modern_tuples[9000]))\n",
    "# print(\"{}\\n{}\\n\".format(agg_original_tuples[18000], agg_modern_tuples[18000]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3710, 2)\n"
     ]
    }
   ],
   "source": [
    "# in-class = original = '1'\n",
    "# raw -> (raw text, label)\n",
    "import numpy as np\n",
    "\n",
    "style1_np = np.array(agg_original_tuples)\n",
    "style1_labels = np.array(['ori' for x in range(len(agg_original_tuples))])\n",
    "X = np.vstack((style1_np, style1_labels))\n",
    "\n",
    "style2_np = np.array(agg_modern_tuples)\n",
    "style2_labels = np.array(['mod' for x in range(len(agg_modern_tuples))])\n",
    "X2 = np.vstack((style2_np, style2_labels))\n",
    "\n",
    "X = np.hstack((X,X2))\n",
    "raw = np.transpose(X)\n",
    "\n",
    "# idx_col = [str(x) for x in range(len(raw))]\n",
    "# idx_col = np.array(idx_col).reshape(-1,1)\n",
    "# print(idx_col.shape)\n",
    "print(raw.shape)\n",
    "#raw = np.hstack((idx_col, raw))\n",
    "raw.shape\n",
    "raw = np.random.permutation(raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[\"You're stripped of your power and your command, and Cassio will govern Cyprus.\",\n",
       "        'mod'],\n",
       "       ['No one passing by?', 'mod'],\n",
       "       ['Therefore, good Emilia, Give me my nightly wearing, and adieu.',\n",
       "        'ori'],\n",
       "       ['What didst not like?', 'ori'],\n",
       "       ['Come, go with me apart.', 'ori'],\n",
       "       ['The voice of Cassio.', 'ori'],\n",
       "       [\"She's so good at sewing, and a wonderful musician.\", 'mod'],\n",
       "       ['Do you perceive in all this noble company Where most you owe obedience?',\n",
       "        'ori'],\n",
       "       ['With any strong or vehement importunity.', 'ori'],\n",
       "       ['The sooner, sweet, for you.', 'ori']], dtype='<U497')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You're stripped of your power and your command, and Cassio will govern Cyprus. gold: mod {'sent_len': 78, 'NOUN_count': 2, 'ADV_count': 0, 'VERB_count': 4, 'ADJ_count': 2, 'adv_verb_ratio': 0, 'adj_noun_ratio': 1.0, 'bleu': 0, 'PRON_count': 1, 'ADP_count': 1, 'CCONJ_count': 2, 'PUNCT_count': 2, 'PROPN_count': 2}\n",
      "\n",
      "list index out of range\n",
      "offending message [], len: 0, row idx: 49\n",
      "Good lieutenant, is your general married? gold: mod {'sent_len': 41, 'NOUN_count': 1, 'ADV_count': 0, 'VERB_count': 1, 'ADJ_count': 4, 'adv_verb_ratio': 0, 'adj_noun_ratio': 0.25, 'bleu': 0, 'PUNCT_count': 2}\n",
      "\n",
      "list index out of range\n",
      "offending message [], len: 0, row idx: 1053\n",
      "Greetings, good general! gold: mod {'sent_len': 24, 'NOUN_count': 2, 'ADV_count': 0, 'VERB_count': 0, 'ADJ_count': 1, 'adv_verb_ratio': 0, 'adj_noun_ratio': 2.0, 'bleu': 0, 'PUNCT_count': 2}\n",
      "\n",
      "Where is that viper? gold: ori {'sent_len': 20, 'NOUN_count': 1, 'ADV_count': 1, 'VERB_count': 1, 'ADJ_count': 0, 'adv_verb_ratio': 1.0, 'adj_noun_ratio': 0, 'bleu': 0, 'DET_count': 1, 'PUNCT_count': 1}\n",
      "\n",
      "(3710, 19)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction import DictVectorizer\n",
    "import spacy\n",
    "import scipy.sparse\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "vectorizer = DictVectorizer()\n",
    "features = []\n",
    "\n",
    "# feature engineering\n",
    "    # construct index column\n",
    "# idx_col = np.array([raw[:,0]])\n",
    "# idx_col = idx_col.astype('f', copy=True)\n",
    "\n",
    "#bleu: 0-1, 1 is perfect score\n",
    "#bleu = sentence_bleu([sen2],sen1)\n",
    "\n",
    "raw_text_col = 0\n",
    "LABEL_COL = -1\n",
    "\n",
    "for row_idx,_ in enumerate(raw):\n",
    "    feature_dict = {}\n",
    "    pos_dict = {'NOUN_count': 0, 'ADV_count': 0, 'VERB_count': 0, \n",
    "                'ADJ_count': 0, 'adv_verb_ratio': 0, 'adj_noun_ratio': 0, 'bleu': 0}\n",
    "    txt = str(raw[row_idx][raw_text_col])\n",
    "    \n",
    "    feature_dict['sent_len'] = len(txt)\n",
    "    try:\n",
    "        doc = nlp(txt)\n",
    "    except IndexError as e:\n",
    "        print(e)\n",
    "        print(\"offending message [{}], len: {}, row idx: {}\".format(txt, len(txt), row_idx))\n",
    "\n",
    "    for token in doc:\n",
    "        pos = token.pos_\n",
    "        pos_dict[pos+'_count'] = pos_dict.get(pos+'_count', 0) + 1\n",
    "\n",
    "    if pos_dict['NOUN_count'] == 0 or pos_dict['ADJ_count'] == 0:\n",
    "        pos_dict['adj_noun_ratio'] = 0\n",
    "    else:\n",
    "        pos_dict['adj_noun_ratio'] = pos_dict['NOUN_count'] / pos_dict['ADJ_count'] \n",
    "\n",
    "    if pos_dict['VERB_count'] == 0 or pos_dict['ADV_count'] == 0:\n",
    "        pos_dict['adv_verb_ratio'] = 0\n",
    "    else:\n",
    "        pos_dict['adv_verb_ratio'] = pos_dict['VERB_count'] / pos_dict['ADV_count'] \n",
    "        \n",
    "    feature_dict.update(pos_dict)\n",
    "    \n",
    "    if row_idx % 1000 == 0:\n",
    "        print(\"{} gold: {} {}\\n\".format(txt, raw[row_idx][LABEL_COL], feature_dict) )\n",
    "        \n",
    "    features.append(feature_dict)\n",
    "\n",
    "#labels_col = np.array([raw[:,LABEL_COL]])\n",
    "\n",
    "X = vectorizer.fit_transform(features)\n",
    "print(X.shape)\n",
    "\n",
    "#X = scipy.sparse.hstack(( idx_col.T, X, labels_col.T ))\n",
    "X = X.todense()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total rows: 3710 train val split delim: 2968 val test split delim: 3339\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "import time\n",
    "import math\n",
    "###############\n",
    "# training ####\n",
    "###############\n",
    "# test breakout: 3500, 210\n",
    "train_val_split_delim = math.floor(len(X) * .8)\n",
    "val_test_split_delim = math.floor(len(X) * .9)\n",
    "print(\"total rows: {} train val split delim: {} val test split delim: {}\".format(len(X), train_val_split_delim, val_test_split_delim))\n",
    "# numpy note: addressing [:delim] is exclusive of delim, but\n",
    "#                         [delim:] is inclusive of delim\n",
    "\n",
    "\n",
    "X_train = X[0:train_val_split_delim]\n",
    "Y_train = raw[0:train_val_split_delim,LABEL_COL]\n",
    "\n",
    "X_val = X[train_val_split_delim:val_test_split_delim]\n",
    "Y_val = raw[train_val_split_delim:val_test_split_delim,LABEL_COL]\n",
    "\n",
    "X_test = X[val_test_split_delim:]\n",
    "Y_test = raw[val_test_split_delim:,LABEL_COL]\n",
    "\n",
    "\n",
    "#mdl = LogisticRegression(random_state=123)\n",
    "mdl = MultinomialNB()\n",
    "mdl.fit(X_train, Y_train)\n",
    "\n",
    "np.set_printoptions(suppress=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['mod', 'ori'], dtype='<U3')"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mdl.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.5067385444743935\n"
     ]
    }
   ],
   "source": [
    "# evaluation ###\n",
    "################\n",
    "#second class is original\n",
    "\n",
    "from sklearn.metrics import brier_score_loss\n",
    "\n",
    "y_true = raw[train_val_split_delim:val_test_split_delim,LABEL_COL]\n",
    "\n",
    "\n",
    "#binary evaluation\n",
    "# preds = mdl.predict(X_test)\n",
    "# \n",
    "# test_indices = X[val_test_split_delim:,0]\n",
    "# if len(preds) != len(val_indices):\n",
    "#     raise ValueError(\"unmatched arrays\")\n",
    "    \n",
    "# results = []\n",
    "# for (idx, pred) in zip(val_indices,preds):\n",
    "#     gold_label = int(raw[int(idx)][-1])\n",
    "#     pred_label = int(pred)\n",
    "#     #print(\"int'd idx: {} gold label: {} pred label: {}\".format(int(idx), gold_label, pred_label))\n",
    "    \n",
    "#     if gold_label == pred_label:\n",
    "#           results.append(1)\n",
    "#     else:\n",
    "#         results.append(0)\n",
    "# results[:5]\n",
    "\n",
    "preds = mdl.predict(X_test)\n",
    "#y_true = [int(raw[int(idx)][-1]) for idx in val_indices]\n",
    "\n",
    "# brier = brier_score_loss(y_true, preds[:,0]) \n",
    "# print(\"brier score:\", brier)\n",
    "\n",
    "#currently slightly worse than randomly permuting preds\n",
    "\n",
    "def accuracy(preds, y_true):\n",
    "    if len(preds) != len(y_true):\n",
    "        raise ArgumentError(\"mismatched arrays!\")\n",
    "    num_correct = 0\n",
    "    for (x,y) in zip(preds, y_true):\n",
    "        if x == y:\n",
    "            num_correct = num_correct + 1\n",
    "    return num_correct/len(preds)\n",
    "\n",
    "print(\"Accuracy: {}\".format(accuracy(preds,y_true)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x1000 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# error analysis ##\n",
    "##################\n",
    "\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "conf_mat = confusion_matrix(y_true, preds)\n",
    "fig, ax = plt.subplots(figsize=(10,10))\n",
    "# sns.heatmap(conf_mat, annot=True, fmt='d',\n",
    "#             xticklabels=category_id_df.Product.values, yticklabels=category_id_df.Product.values)\n",
    "plt.ylabel('Actual')\n",
    "plt.xlabel('Predicted')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_len = len(preds)\n",
    "# accuracy = sum(preds)/float(test_len)\n",
    "# print(\"accuracy: {}\".format(accuracy))\n",
    "\n",
    "#othello accuracy: 0.7428571428571429. but maybe not permuted\n",
    "#all: accuracy: 0.328237942884228\n",
    "\n",
    "#pos, no bleu, 2 plays\n",
    "# Mult NB: .519"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
